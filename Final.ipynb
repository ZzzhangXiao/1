{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline as make_imb_pipeline\n",
    "from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from pympler import tracker\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('evaluation round1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12604600, 69)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ind_recommended</th>\n",
       "      <th>activation</th>\n",
       "      <th>customer_digital_activity_04</th>\n",
       "      <th>customer_spend_01</th>\n",
       "      <th>customer_industry_spend_01</th>\n",
       "      <th>customer_industry_spend_02</th>\n",
       "      <th>customer_industry_spend_03</th>\n",
       "      <th>customer_industry_spend_04</th>\n",
       "      <th>customer_industry_spend_05</th>\n",
       "      <th>customer_spend_02</th>\n",
       "      <th>...</th>\n",
       "      <th>merchant_spend_09</th>\n",
       "      <th>merchant_profile_03</th>\n",
       "      <th>customer_digital_activity_01</th>\n",
       "      <th>merchant_spend_10</th>\n",
       "      <th>customer_profile_03</th>\n",
       "      <th>customer_digital_activity_02</th>\n",
       "      <th>customer_profile_04</th>\n",
       "      <th>distance_05</th>\n",
       "      <th>customer</th>\n",
       "      <th>merchant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107.215862</td>\n",
       "      <td>26.686594</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3682.75</td>\n",
       "      <td>138.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>49466.0</td>\n",
       "      <td>65923.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.180</td>\n",
       "      <td>58.434969</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>86.0</td>\n",
       "      <td>15.856826</td>\n",
       "      <td>168972</td>\n",
       "      <td>152285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.552000</td>\n",
       "      <td>50.928261</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1171.35</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3638.0</td>\n",
       "      <td>7801.0</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>28.465</td>\n",
       "      <td>5.392089</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>125.0</td>\n",
       "      <td>6.998555</td>\n",
       "      <td>212404</td>\n",
       "      <td>39032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.623103</td>\n",
       "      <td>48.837872</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2295.38</td>\n",
       "      <td>47.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3912.0</td>\n",
       "      <td>12868.0</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>421.500</td>\n",
       "      <td>33.780445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.753009</td>\n",
       "      <td>225178</td>\n",
       "      <td>7439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112.277391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28919.0</td>\n",
       "      <td>23553.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>50.000</td>\n",
       "      <td>37.340085</td>\n",
       "      <td>28.666667</td>\n",
       "      <td>134.0</td>\n",
       "      <td>9.000063</td>\n",
       "      <td>183948</td>\n",
       "      <td>485069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>448.427273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1086.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>69.509</td>\n",
       "      <td>77.794164</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1.767939</td>\n",
       "      <td>210107</td>\n",
       "      <td>536004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ind_recommended  activation  customer_digital_activity_04  \\\n",
       "0                0           0                           NaN   \n",
       "1                0           0                           NaN   \n",
       "2                0           0                           NaN   \n",
       "3                0           0                           NaN   \n",
       "4                1           0                           NaN   \n",
       "\n",
       "   customer_spend_01  customer_industry_spend_01  customer_industry_spend_02  \\\n",
       "0         107.215862                   26.686594                        74.0   \n",
       "1          35.552000                   50.928261                         3.0   \n",
       "2          31.623103                   48.837872                        19.0   \n",
       "3         112.277391                         NaN                         NaN   \n",
       "4         448.427273                         NaN                         NaN   \n",
       "\n",
       "   customer_industry_spend_03  customer_industry_spend_04  \\\n",
       "0                     3682.75                       138.0   \n",
       "1                     1171.35                        23.0   \n",
       "2                     2295.38                        47.0   \n",
       "3                         NaN                         NaN   \n",
       "4                         NaN                         NaN   \n",
       "\n",
       "   customer_industry_spend_05  customer_spend_02  ...  merchant_spend_09  \\\n",
       "0                       111.0               14.0  ...            49466.0   \n",
       "1                        17.0                2.0  ...             3638.0   \n",
       "2                        42.0               11.0  ...             3912.0   \n",
       "3                         NaN               16.0  ...            28919.0   \n",
       "4                         NaN                5.0  ...             1086.0   \n",
       "\n",
       "   merchant_profile_03  customer_digital_activity_01  merchant_spend_10  \\\n",
       "0              65923.0                      0.000000             29.180   \n",
       "1               7801.0                      0.419355             28.465   \n",
       "2              12868.0                      0.836364            421.500   \n",
       "3              23553.0                      0.952381             50.000   \n",
       "4                308.0                      0.754386             69.509   \n",
       "\n",
       "   customer_profile_03  customer_digital_activity_02  customer_profile_04  \\\n",
       "0            58.434969                     32.500000                 86.0   \n",
       "1             5.392089                      7.000000                125.0   \n",
       "2            33.780445                      0.000000                180.0   \n",
       "3            37.340085                     28.666667                134.0   \n",
       "4            77.794164                     15.000000                114.0   \n",
       "\n",
       "   distance_05  customer  merchant  \n",
       "0    15.856826    168972    152285  \n",
       "1     6.998555    212404     39032  \n",
       "2     1.753009    225178      7439  \n",
       "3     9.000063    183948    485069  \n",
       "4     1.767939    210107    536004  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for column 'activation':\n",
      "activation\n",
      "0    12159962\n",
      "1       70016\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "size = df.shape\n",
    "valid_counts = df.count()\n",
    "\n",
    "column_of_interest = 'activation'\n",
    "\n",
    "# Get the unique values and the number of unique values\n",
    "unique_values = df[column_of_interest].unique()\n",
    "value_counts = df[column_of_interest].value_counts()\n",
    "\n",
    "print(\"Value counts for column '{}':\\n{}\".format(column_of_interest, value_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows\n",
    "major issue of missing values in multiple entires. if eliminate all NA values, the data will have small size\\\n",
    "drop rows with more than half the features being NA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping those with more than half the data dimension being na, the size is: (7822511, 71)\n"
     ]
    }
   ],
   "source": [
    "drop_na  = df.dropna(thresh=len(df.columns)/2)\n",
    "del df # save memories\n",
    "print ('After dropping those with more than half the data dimension being na, the size is:', drop_na.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns\n",
    "customer_merchant_01, customer_merchant_02, customer_digital_activity_01, 02, 07, 08, 09, 18\\\n",
    "Assumption: they do not contribute significantly to the recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = drop_na.activation\n",
    "columns_to_drop = []\n",
    "for col in drop_na.columns:\n",
    "    if drop_na[col].count() < size[0]*0.1:\n",
    "        columns_to_drop.append(col)\n",
    "\n",
    "data_train = drop_na.drop(columns_to_drop, axis=1)\n",
    "\n",
    "\n",
    "del drop_na\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train.drop(['ind_recommended', 'activation'],axis=1)\n",
    "\n",
    "del data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imbalanced UnderSampling: \n",
    "Choice of undersampling technique: Random Undersampling\\\n",
    "https://imbalanced-learn.org/stable/under_sampling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((134120, 61),\n",
       " activation\n",
       " 0    67060\n",
       " 1    67060\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   EditedN\n",
    "##  enn = EditedNearestNeighbours()\n",
    "#   X_resampled, y_resampled = enn.fit_resample(X_train, y_train)\n",
    "\n",
    "##  renn = RepeatedEditedNearestNeighbours()\n",
    "#   X_resampled, y_resampled = renn.fit_resample(X_train, y_train)\n",
    "rus = RandomUnderSampler()\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "class_count = y_resampled.value_counts()\n",
    "X_resampled.shape, class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_imputer = KNNImputer(n_neighbors=5)  # Set the number of neighbors as per your dataset\n",
    "X_imputed = knn_imputer.fit_transform(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in the array:  0\n"
     ]
    }
   ],
   "source": [
    "nan_count = np.isnan(X_imputed).sum()\n",
    "print(\"Number of NaN values in the array: \", nan_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serialize for training data use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the train and test data ready for continued use training models \n",
    "data = {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}\n",
    "with open('amex_traintest.pickle', 'wb') as file:\n",
    "    pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data (run this everytime the kernel is restarted)\n",
    "with open('amex_traintest.pickle', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    X_train = data['X_train']\n",
    "    X_test = data['X_test']\n",
    "    y_train = data['y_train']\n",
    "    y_test = data['y_test']\n",
    "\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((107296, 61), (26824, 61), (107296,), (26824,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_resampled is the undersampled feature matrix\n",
    "\n",
    "# Define the parameter grid for number of neighbors\n",
    "param_grid = {'n_neighbors': [3, 5, 7, 9, 11]}  \n",
    "\n",
    "# Instantiate the KNNImputer\n",
    "knn_imputer = KNNImputer()\n",
    "\n",
    "# Perform grid search to find the optimal number of neighbors\n",
    "grid_search = GridSearchCV(knn_imputer, param_grid, scoring='roc_auc', cv=5)\n",
    "grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Best number of neighbors\n",
    "best_neighbors = grid_search.best_params_['n_neighbors']\n",
    "print(f\"Optimal number of neighbors: {best_neighbors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(X_train, X_test):\n",
    "    sc = StandardScaler()\n",
    "    X_train_ss = sc.fit_transform(X_train)\n",
    "    X_test_ss = sc.transform(X_test)\n",
    "    return X_train_ss, X_test_ss\n",
    "X_train_ss, X_test_ss = scale_features(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8137424902561045,\n",
       " array([[10925,  2577],\n",
       "        [ 2420, 10902]], dtype=int64))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RandomForest Classifier\n",
    "RF = RandomForestClassifier(random_state=42)\n",
    "RF.fit(X_train, y_train)\n",
    "y_pred_test_rf = cross_val_predict(RF, X_test, y_test, cv=5, method='predict')\n",
    "roc_auc_score(y_test, y_pred_test_rf), confusion_matrix(y_test, y_pred_test_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7036878454522221,\n",
       " array([[9597, 3905],\n",
       "        [4042, 9280]], dtype=int64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVC: non standardized\n",
    "svm = SVC(random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_test_svm = cross_val_predict(svm, X_test, y_test, cv=5, method = \"predict\")\n",
    "roc_auc_score(y_test, y_pred_test_svm), confusion_matrix(y_test, y_pred_test_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8010196591113704,\n",
       " array([[10699,  2803],\n",
       "        [ 2536, 10786]], dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVC: distanced based so we use X_train_ss\n",
    "svm = SVC(random_state=42)\n",
    "svm.fit(X_train_ss, y_train)\n",
    "y_pred_test_svm = cross_val_predict(svm, X_test_ss, y_test, cv=5, method = \"predict\")\n",
    "roc_auc_score(y_test, y_pred_test_svm), confusion_matrix(y_test, y_pred_test_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.654216762295648,\n",
       " array([[9155, 4347],\n",
       "        [4924, 8398]], dtype=int64))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5) \n",
    "knn.fit(X_train, y_train)  \n",
    "y_pred_test_knn = cross_val_predict(knn, X_test, y_test, cv=5, method='predict')\n",
    "roc_auc_score(y_test, y_pred_test_knn), confusion_matrix(y_test, y_pred_test_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7260558695302798,\n",
       " array([[9749, 3753],\n",
       "        [3596, 9726]], dtype=int64))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN Classifier: Standardized\n",
    "knn = KNeighborsClassifier(n_neighbors=5) \n",
    "knn.fit(X_train_ss, y_train)  \n",
    "y_pred_test_knn = cross_val_predict(knn, X_test_ss, y_test, cv=5, method='predict')\n",
    "roc_auc_score(y_test, y_pred_test_knn), confusion_matrix(y_test, y_pred_test_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble: Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf = 0.821391291380853\n",
      "svc = 0.8092752758723532\n",
      "knn = 0.7385923053981509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8129287205487623"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Voting Classifier with all exisiting classifier - hard voting\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        #('lr', LogisticRegression(random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('svc', SVC(random_state=42)),\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "    ]\n",
    ")\n",
    "voting_clf.fit(X_train_ss, y_train)\n",
    "\n",
    "for name, clf in voting_clf.named_estimators_.items():\n",
    "    print (name, \"=\", clf.score(X_test_ss, y_test))\n",
    "voting_clf.score(X_test_ss, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81031911720847"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Soft voting\n",
    "voting_clf.voting = \"soft\"\n",
    "voting_clf.named_estimators[\"svc\"].probability = True\n",
    "voting_clf.fit(X_train_ss, y_train)\n",
    "voting_clf.score(X_test_ss, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and split into features and target variable\n",
    "# Assuming X and y are the features and target variable\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Instantiate the Random Forest Classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the Best Parameters\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the Model\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "# Additional evaluation metrics and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'criterion': 'gini',\n",
       "  'max_depth': None,\n",
       "  'min_samples_split': 5,\n",
       "  'n_estimators': 300},\n",
       " RandomForestClassifier(min_samples_split=5, n_estimators=300))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params, best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the best y, estimator, params ready for continued use training models \n",
    "params = {'y_pred': y_pred, 'best_estimator': best_estimator, 'best_params': best_params}\n",
    "with open('amex_GridsearchRF.pickle', 'wb') as file:\n",
    "    pickle.dump(params, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.823046399171187,\n",
       " array([[10933,  2569],\n",
       "        [ 2180, 11142]], dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RF: Best y, parameter, and estimator\n",
    "roc_auc_score(y_test, y_pred), confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Instantiate the Support Vector Classifier (SVC)\n",
    "svc = SVC()\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train_ss, y_train)\n",
    "\n",
    "# Retrieve the Best Parameters\n",
    "best_params_svc = grid_search.best_params_\n",
    "best_estimator_svc = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the Model\n",
    "y_pred_svc = best_estimator_svc.predict(X_test_ss)\n",
    "# Additional evaluation metrics and analysis\n",
    "roc_auc_score(y_test, y_pred_svc), confusion_matrix(y_test, y_pred_svc), best_params_svc, best_estimator_svc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-Standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8101699970175962"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=[5], max_iter=10_000,\n",
    "                        random_state=42)\n",
    "pipeline = make_pipeline(StandardScaler(), mlp_clf)\n",
    "pipeline.fit(X_train, y_train)\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8101699970175962"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp_clf = MLPClassifier(hidden_layer_sizes=[5], max_iter=10_000,\n",
    "                        random_state=42)\n",
    "pipeline = make_pipeline(StandardScaler(), mlp_clf)\n",
    "pipeline.fit(X_train_ss, y_train)\n",
    "accuracy = pipeline.score(X_test_ss, y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Tuner network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=30, default=2)\n",
    "    n_neurons = hp.Int(\"n_neurons\", min_value=16, max_value=800)\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2,\n",
    "                             sampling=\"log\")\n",
    "    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_tuner = kt.RandomSearch(\n",
    "    build_model, objective=\"val_accuracy\", max_trials=5, overwrite=True,\n",
    "    directory=\"Amex Hackathon\", project_name=\"recommendation\", seed=42)\n",
    "random_search_tuner.search(X_train_ss, y_train, epochs=100,\n",
    "                           validation_data=(X_test_ss, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_models = random_search_tuner.get_best_models(num_models=3)\n",
    "best_model = top3_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_params = random_search_tuner.get_best_hyperparameters(num_trials=3)\n",
    "top3_params[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = random_search_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "best_trial.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Normalization()\n",
    "hidden_layer1 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "hidden_layer2 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "concat_layer = tf.keras.layers.Concatenate()\n",
    "output_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "input_ = tf.keras.layers.Input(shape=X_train.shape[1:])\n",
    "normalized = normalization_layer(input_)\n",
    "hidden1 = hidden_layer1(normalized)\n",
    "hidden2 = hidden_layer2(hidden1)\n",
    "concat = concat_layer([normalized, hidden2])\n",
    "output = output_layer(concat)\n",
    "\n",
    "model_xxx = tf.keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.6779 - loss: 3.0317\n",
      "Epoch 2/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7195 - loss: 1.5214\n",
      "Epoch 3/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7275 - loss: 0.9006\n",
      "Epoch 4/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.7007 - loss: 1.0449\n",
      "Epoch 5/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.6791 - loss: 1.0379\n",
      "Epoch 6/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7388 - loss: 0.7347\n",
      "Epoch 7/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7497 - loss: 0.6611\n",
      "Epoch 8/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7661 - loss: 0.6374\n",
      "Epoch 9/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7717 - loss: 0.5894\n",
      "Epoch 10/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.7499 - loss: 0.6764\n",
      "Epoch 11/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7622 - loss: 0.6217\n",
      "Epoch 12/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7807 - loss: 0.5651\n",
      "Epoch 13/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7775 - loss: 0.5731\n",
      "Epoch 14/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7722 - loss: 0.6062\n",
      "Epoch 15/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7842 - loss: 0.5530\n",
      "Epoch 16/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7757 - loss: 0.6066\n",
      "Epoch 17/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7581 - loss: 0.7045\n",
      "Epoch 18/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.7929 - loss: 0.5768\n",
      "Epoch 19/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.7799 - loss: 0.5850\n",
      "Epoch 20/20\n",
      "\u001b[1m3353/3353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.7913 - loss: 0.5333\n",
      "\u001b[1m839/839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7591 - loss: 0.5956\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model_xxx.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "                  metrics=[\"accuracy\"])\n",
    "normalization_layer.adapt(X_train)\n",
    "history = model_xxx.fit(X_train, y_train, epochs=20)\n",
    "score = model_xxx.evaluate(X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee2211",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "0927f12649cd6126d2b951ca618f7a887eb1ed8413a3efcfff607a99994a5067"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
